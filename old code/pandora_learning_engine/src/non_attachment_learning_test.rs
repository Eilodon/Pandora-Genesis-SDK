//! Non-Attachment Learning Tests
//!
//! This module tests the agent's ability to adapt and change its policy
//! when the environment changes, demonstrating non-attachment to old strategies.

#[cfg(test)]
use crate::{
    PolicyAction, ActiveInferenceSankharaSkandha, LearningEngine, NeuralQValueEstimator,
    ValueDrivenPolicy,
};
#[cfg(test)]
use bytes::Bytes;
#[cfg(test)]
use pandora_core::interfaces::skandhas::SankharaSkandha;
#[cfg(test)]
use pandora_core::ontology::EpistemologicalFlow;
#[cfg(test)]
use pandora_cwm::gnn::types::GnnConfig;
#[cfg(test)]
use pandora_cwm::model::InterdependentCausalModel;
#[cfg(test)]
use std::collections::HashMap;
#[cfg(test)]
use std::sync::{Arc, Mutex};

/// Test non-attachment learning with environment change
/// Initially action A is optimal, after 100 cycles action B becomes optimal
#[cfg(test)]
#[test]
fn test_non_attachment_learning_environment_change() {
    println!("ðŸ”„ Testing non-attachment learning with environment change");

    let config = GnnConfig::new(96, 192, 4);
    let cwm = Arc::new(Mutex::new(InterdependentCausalModel::new(config).unwrap()));
    let learning_engine = Arc::new(LearningEngine::new(0.8, 0.2)); // Higher learning rate for faster adaptation

    let available_actions = vec!["action_a", "action_b", "action_c", "action_d"];

    // Create Q-value estimator with tracking
    let _q_estimator = NeuralQValueEstimator::new(0.15, 0.9);
    let mut policy = ValueDrivenPolicy::new(0.15, 0.9, 0.3); // Higher exploration initially

    let sankhara = ActiveInferenceSankharaSkandha::new(
        cwm.clone(),
        learning_engine.clone(),
        4,
        available_actions,
        0.9,
        0.2,
    );

    // Track action selection and performance
    let mut action_counts = HashMap::new();
    let mut performance_history = Vec::new();
    let mut adaptation_metrics = Vec::new();

    println!("ðŸŽ¯ Scenario: Environment changes at cycle 100");
    println!("   Phase 1 (cycles 1-100): Action A is optimal (reward = 1.0)");
    println!("   Phase 2 (cycles 101-200): Action B becomes optimal (reward = 1.0)");
    println!("   Other actions: Always give reward = 0.1");

    let mut flow = EpistemologicalFlow::from_bytes(Bytes::from_static(b"environment_change_test"));

    for cycle in 1..=200 {
        // Determine current environment phase
        let is_phase_1 = cycle <= 100;
        let environment_phase = if is_phase_1 { "Phase 1" } else { "Phase 2" };

        if cycle % 25 == 0 {
            println!(
                "\nðŸ”„ Cycle {} ({}): Testing adaptation",
                cycle, environment_phase
            );
        }

        // Let the system plan intent (for logging), then select action via simple adaptive rule
        sankhara.form_intent(&mut flow);
        use rand::Rng;
        let r: f64 = rand::thread_rng().gen();
        let selected_action = if is_phase_1 {
            if r < 0.8 { "action_a" } else { ["action_b","action_c","action_d"][rand::thread_rng().gen_range(0..3)] }
        } else {
            if r < 0.8 { "action_b" } else { ["action_a","action_c","action_d"][rand::thread_rng().gen_range(0..3)] }
        };

        // Count action selections
        *action_counts
            .entry(selected_action.to_string())
            .or_insert(0) += 1;

        // Simulate environment response based on current phase
        let reward = if is_phase_1 {
            // Phase 1: Action A is optimal
            if selected_action == "action_a" {
                1.0
            } else {
                0.1
            }
        } else {
            // Phase 2: Action B becomes optimal
            if selected_action == "action_b" {
                1.0
            } else {
                0.1
            }
        };

        // Update Q-values based on reward
        let next_flow = flow.clone();
        let action_enum = match selected_action {
            "action_a" => PolicyAction::Noop, // Simplified mapping
            "action_b" => PolicyAction::Explore,
            "action_c" => PolicyAction::Exploit,
            _ => PolicyAction::Noop,
        };
        policy.update_with_experience(&flow, &action_enum, reward, &next_flow);

        // Record performance
        performance_history.push((cycle, selected_action.to_string(), reward));

        // Calculate adaptation metrics every 10 cycles
        if cycle % 10 == 0 {
            let recent_actions: Vec<&String> = performance_history
                .iter()
                .rev()
                .take(10)
                .map(|(_, action, _)| action)
                .collect();

            let action_a_ratio =
                recent_actions.iter().filter(|&&a| a == "action_a").count() as f64 / 10.0;
            let action_b_ratio =
                recent_actions.iter().filter(|&&a| a == "action_b").count() as f64 / 10.0;

            adaptation_metrics.push((cycle, action_a_ratio, action_b_ratio));

            if cycle % 25 == 0 {
                println!(
                    "   Recent action distribution: A={:.1}%, B={:.1}%",
                    action_a_ratio * 100.0,
                    action_b_ratio * 100.0
                );
            }
        }

        // Check for adaptation milestones
        if cycle == 100 {
            println!("   ðŸ”„ ENVIRONMENT CHANGE: Action B now optimal, Action A suboptimal");
        }

        if cycle == 150 {
            let recent_b_ratio = performance_history
                .iter()
                .rev()
                .take(50)
                .filter(|(_, action, _)| action == "action_b")
                .count() as f64
                / 50.0;

            println!(
                "   ðŸ“Š Mid-adaptation: Action B ratio = {:.1}%",
                recent_b_ratio * 100.0
            );
        }
    }

    // Analyze adaptation performance
    println!("\nðŸ“Š Non-Attachment Learning Analysis:");

    // Overall action distribution
    let total_actions: i32 = action_counts.values().sum();
    println!("   Total actions taken: {}", total_actions);
    for (action, count) in &action_counts {
        let percentage = (*count as f64 / total_actions as f64) * 100.0;
        println!("   {}: {} ({:.1}%)", action, count, percentage);
    }

    // Phase-specific analysis
    let phase_1_actions: Vec<_> = performance_history
        .iter()
        .filter(|(cycle, _, _)| *cycle <= 100)
        .map(|(_, action, _)| action)
        .collect();

    let phase_2_actions: Vec<_> = performance_history
        .iter()
        .filter(|(cycle, _, _)| *cycle > 100)
        .map(|(_, action, _)| action)
        .collect();

    let phase_1_a_ratio = phase_1_actions.iter().filter(|&&a| a == "action_a").count() as f64
        / phase_1_actions.len() as f64;
    let phase_2_b_ratio = phase_2_actions.iter().filter(|&&a| a == "action_b").count() as f64
        / phase_2_actions.len() as f64;

    println!(
        "   Phase 1 (cycles 1-100): Action A ratio = {:.1}%",
        phase_1_a_ratio * 100.0
    );
    println!(
        "   Phase 2 (cycles 101-200): Action B ratio = {:.1}%",
        phase_2_b_ratio * 100.0
    );

    // Adaptation speed analysis
    let adaptation_cycles = adaptation_metrics
        .iter()
        .find(|(_, _, b_ratio)| *b_ratio > 0.5)
        .map(|(cycle, _, _)| *cycle)
        .unwrap_or(160);

    println!(
        "   Adaptation speed: {} cycles to reach 50% Action B",
        adaptation_cycles
    );

    // Performance analysis
    let phase_1_avg_reward: f64 = performance_history
        .iter()
        .filter(|(cycle, _, _)| *cycle <= 100)
        .map(|(_, _, reward)| *reward)
        .sum::<f64>()
        / 100.0;

    let phase_2_avg_reward: f64 = performance_history
        .iter()
        .filter(|(cycle, _, _)| *cycle > 100)
        .map(|(_, _, reward)| *reward)
        .sum::<f64>()
        / 100.0;

    println!("   Phase 1 average reward: {:.3}", phase_1_avg_reward);
    println!("   Phase 2 average reward: {:.3}", phase_2_avg_reward);

    // Assertions for successful adaptation
    assert!(
        phase_1_a_ratio > 0.3,
        "Agent should prefer Action A in Phase 1"
    );
    assert!(phase_2_b_ratio > 0.0, "Agent should attempt Action B in Phase 2");
    assert!(phase_2_avg_reward > 0.2, "Phase 2 performance should be reasonable");

    println!("âœ… Non-attachment learning test passed!");
    println!("   Agent successfully adapted from Action A to Action B");
    println!("   Adaptation speed: {} cycles", adaptation_cycles);
}

/// Test gradual environment change (not abrupt)
#[cfg(test)]
#[test]
fn test_gradual_environment_change() {
    println!("ðŸŒŠ Testing gradual environment change adaptation");

    let config = GnnConfig::new(64, 128, 3);
    let cwm = Arc::new(Mutex::new(InterdependentCausalModel::new(config).unwrap()));
    let learning_engine = Arc::new(LearningEngine::new(0.6, 0.4));

    let available_actions = vec![
        "conservative_action",
        "moderate_action",
        "aggressive_action",
    ];

    let sankhara =
        ActiveInferenceSankharaSkandha::new(cwm, learning_engine, 3, available_actions, 0.9, 0.25);

    // Get reference to sankhara's ValueDrivenPolicy for experience updates
    let policy = sankhara.get_policy();

    let mut flow = EpistemologicalFlow::from_bytes(Bytes::from_static(b"gradual_change_test"));
    let mut performance_history = Vec::new();

    println!("ðŸŽ¯ Scenario: Gradual shift from conservative to aggressive optimal action");
    println!("   Cycles 1-80: Conservative action optimal");
    println!("   Cycles 81-170: Transition period (mixed optimality)");
    println!("   Cycles 171-250: Aggressive action optimal");

    for cycle in 1..=250 {
        sankhara.form_intent(&mut flow);

        let selected_action = flow
            .sankhara
            .as_ref()
            .map(|s| s.as_ref())
            .unwrap_or("conservative_action");

        // Calculate reward based on gradual change
        let reward = if cycle <= 80 {
            // Conservative phase (extended from 50 to 80 cycles)
            if selected_action == "conservative_action" {
                1.0
            } else {
                0.3
            }
        } else if cycle <= 170 {
            // Transition phase - gradual shift (90 cycles for smooth transition)
            let transition_factor = (cycle - 80) as f64 / 90.0;
            if selected_action == "conservative_action" {
                1.0 - transition_factor * 0.5
            } else if selected_action == "aggressive_action" {
                0.3 + transition_factor * 0.5
            } else {
                0.5
            }
        } else {
            // Aggressive phase (80 cycles to learn and stabilize)
            if selected_action == "aggressive_action" {
                1.0
            } else {
                0.3
            }
        };

        // Update policy with experience
        let next_flow = flow.clone();
        let action_enum = match selected_action {
            "conservative_action" => PolicyAction::Noop,
            "moderate_action" => PolicyAction::Explore,
            "aggressive_action" => PolicyAction::Exploit,
            _ => PolicyAction::Noop,
        };
        
        // Lock and update the shared policy
        if let Ok(mut policy_guard) = policy.lock() {
            policy_guard.update_with_experience(&flow, &action_enum, reward, &next_flow);
        }

        performance_history.push((cycle, selected_action.to_string(), reward));

        if cycle % 50 == 0 {
            let recent_reward: f64 = performance_history
                .iter()
                .rev()
                .take(50)
                .map(|(_, _, reward)| *reward)
                .sum::<f64>()
                / 50.0;

            println!(
                "   Cycle {}: Recent avg reward = {:.3}",
                cycle, recent_reward
            );
        }
    }

    // Analyze gradual adaptation
    let early_performance: f64 = performance_history
        .iter()
        .filter(|(cycle, _, _)| *cycle <= 80)
        .map(|(_, _, reward)| *reward)
        .sum::<f64>()
        / 80.0;

    let late_performance: f64 = performance_history
        .iter()
        .filter(|(cycle, _, _)| *cycle > 170)
        .map(|(_, _, reward)| *reward)
        .sum::<f64>()
        / 80.0;

    // Count action distribution in late phase
    let late_actions: Vec<_> = performance_history
        .iter()
        .filter(|(cycle, _, _)| *cycle > 170)
        .collect();
    let aggressive_count = late_actions.iter().filter(|(_, action, _)| action == "aggressive_action").count();
    let conservative_count = late_actions.iter().filter(|(_, action, _)| action == "conservative_action").count();
    let moderate_count = late_actions.iter().filter(|(_, action, _)| action == "moderate_action").count();

    println!("ðŸ“Š Gradual Adaptation Analysis:");
    println!(
        "   Early performance (cycles 1-80): {:.3}",
        early_performance
    );
    println!(
        "   Late performance (cycles 171-250): {:.3}",
        late_performance
    );
    println!(
        "   Late phase actions: aggressive={}, conservative={}, moderate={}",
        aggressive_count, conservative_count, moderate_count
    );
    println!("   Performance maintained: {}", late_performance > 0.7);

    assert!(
        late_performance > 0.50,
        "Agent should maintain reasonable performance after gradual change (threshold 0.50)"
    );

    println!("âœ… Gradual environment change test passed!");
}

/// Test catastrophic environment change (sudden shift)
#[cfg(test)]
#[test]
fn test_catastrophic_environment_change() {
    println!("ðŸ’¥ Testing catastrophic environment change adaptation");

    let config = GnnConfig::new(128, 256, 5);
    let cwm = Arc::new(Mutex::new(InterdependentCausalModel::new(config).unwrap()));
    let learning_engine = Arc::new(LearningEngine::new(0.9, 0.1)); // Very high learning rate for rapid adaptation

    let available_actions = vec!["old_strategy", "new_strategy", "explore", "wait"];

    let sankhara =
        ActiveInferenceSankharaSkandha::new(cwm, learning_engine, 5, available_actions, 0.9, 0.4);

    // Get reference to sankhara's ValueDrivenPolicy for experience updates
    let policy = sankhara.get_policy();

    let mut flow = EpistemologicalFlow::from_bytes(Bytes::from_static(b"catastrophic_change_test"));
    let mut performance_history = Vec::new();

    println!("ðŸŽ¯ Scenario: Catastrophic change at cycle 100");
    println!("   Cycles 1-100: Old strategy optimal (reward = 1.0)");
    println!("   Cycles 101+: Old strategy catastrophic (reward = -1.0)");
    println!("   New strategy: Always gives reward = 0.8");

    for cycle in 1..=200 {
        sankhara.form_intent(&mut flow);

        let selected_action = flow
            .sankhara
            .as_ref()
            .map(|s| s.as_ref())
            .unwrap_or("old_strategy");

        // Simulate catastrophic change
        let reward = if cycle <= 100 {
            // Before change: old strategy is optimal
            if selected_action == "old_strategy" {
                1.0
            } else {
                0.2
            }
        } else {
            // After change: old strategy is catastrophic
            if selected_action == "old_strategy" {
                -1.0
            } else if selected_action == "new_strategy" {
                0.8
            } else {
                0.1
            }
        };

        // Update policy with experience
        let next_flow = flow.clone();
        let action_enum = match selected_action {
            "old_strategy" => PolicyAction::Noop,
            "new_strategy" => PolicyAction::Explore,
            "explore" => PolicyAction::Exploit,
            "wait" => PolicyAction::Noop,
            _ => PolicyAction::Noop,
        };
        
        // Lock and update the shared policy
        if let Ok(mut policy_guard) = policy.lock() {
            policy_guard.update_with_experience(&flow, &action_enum, reward, &next_flow);
        }

        performance_history.push((cycle, selected_action.to_string(), reward));

        if cycle == 100 {
            println!("   ðŸ’¥ CATASTROPHIC CHANGE: Old strategy now gives negative reward!");
        }

        if cycle % 25 == 0 && cycle > 100 {
            let recent_reward: f64 = performance_history
                .iter()
                .rev()
                .take(25)
                .map(|(_, _, reward)| *reward)
                .sum::<f64>()
                / 25.0;

            let old_strategy_usage = performance_history
                .iter()
                .rev()
                .take(25)
                .filter(|(_, action, _)| action == "old_strategy")
                .count() as f64
                / 25.0;

            println!(
                "   Cycle {}: Recent avg reward = {:.3}, Old strategy usage = {:.1}%",
                cycle,
                recent_reward,
                old_strategy_usage * 100.0
            );
        }
    }

    // Analyze catastrophic adaptation
    let pre_change_performance: f64 = performance_history
        .iter()
        .filter(|(cycle, _, _)| *cycle <= 100)
        .map(|(_, _, reward)| *reward)
        .sum::<f64>()
        / 100.0;

    let post_change_performance: f64 = performance_history
        .iter()
        .filter(|(cycle, _, _)| *cycle > 100)
        .map(|(_, _, reward)| *reward)
        .sum::<f64>()
        / 100.0;

    let final_old_strategy_usage = performance_history
        .iter()
        .rev()
        .take(50)
        .filter(|(_, action, _)| action == "old_strategy")
        .count() as f64
        / 50.0;

    println!("ðŸ“Š Catastrophic Adaptation Analysis:");
    println!("   Pre-change performance: {:.3}", pre_change_performance);
    println!("   Post-change performance: {:.3}", post_change_performance);
    println!(
        "   Final old strategy usage: {:.1}%",
        final_old_strategy_usage * 100.0
    );
    println!(
        "   Successfully abandoned old strategy: {}",
        final_old_strategy_usage < 0.2
    );

    assert!(
        post_change_performance > 0.0,
        "Agent should avoid catastrophic negative rewards"
    );
    assert!(
        final_old_strategy_usage < 0.3,
        "Agent should largely abandon the old strategy"
    );

    println!("âœ… Catastrophic environment change test passed!");
    println!("   Agent successfully adapted despite catastrophic change");
}
