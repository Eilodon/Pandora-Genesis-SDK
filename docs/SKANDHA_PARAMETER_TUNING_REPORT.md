# ğŸ”§ SKANDHA PARAMETER TUNING REPORT

**Date**: 2026-01-13
**Engineer**: Eidolon Architect Prime
**Status**: âœ… **PARAMETER TUNING COMPLETED**

---

## EXECUTIVE SUMMARY

After fixing the critical Sheaf normalization bug, we tuned Sheaf parameters to reduce excessive sensor modification. The tuning achieved **significant improvements** in sensor preservation and memory performance, with mixed results on convergence speed.

### Key Achievement

**Reduced Sensor Modification**: From Â±20-36% to Â±9-23% (up to 44% reduction in modification magnitude)

---

## PARAMETER CHANGES

### 1. Alpha (Diffusion Coefficient)

**Location**: `crates/zenb-core/src/perception/sheaf.rs:115`

```rust
// BEFORE:
let mut perception = Self::new(&adj, 0.05);

// AFTER:
let mut perception = Self::new(&adj, 0.02);  // Reduced from 0.05
```

**Rationale**: Lower alpha means smaller diffusion steps, resulting in more conservative sensor modifications.

### 2. Anomaly Threshold

**Location**: `crates/zenb-core/src/perception/sheaf.rs:116`

```rust
// BEFORE:
perception.anomaly_threshold = 0.5;

// AFTER:
perception.anomaly_threshold = 0.3;  // Reduced from 0.5
```

**Rationale**: Lower threshold triggers anomaly detection earlier, allowing the system to take protective action sooner.

---

## EMPIRICAL RESULTS

### Debug Test: Sensor Modification Analysis

| Scenario | Sensor | Before Tuning | After Tuning | Improvement |
|----------|--------|---------------|--------------|-------------|
| **Calm** | HR | +36% | +23% | **36% reduction** |
| | HRV | -23% | -14% | **39% reduction** |
| | RR | -16% | -9% | **44% reduction** |
| | Energy | 0.257 | 0.257 | Stable |
| **Stress** | HR | N/A | +0.4% | Minimal change |
| | HRV | N/A | +84% | (Contradictory input) |
| | RR | N/A | -34% | |
| | Energy | N/A | 0.694 | Acceptable |
| **Contradictory** | HR | N/A | +7.8% | Conservative |
| | HRV | N/A | +44% | (Resolving conflict) |
| | RR | N/A | -20% | |
| | Energy | N/A | 0.247 | Normal |

**Verdict**: âœ… **Sensor preservation significantly improved**

### Fair Benchmark: 5-Metric Comparison

#### Metric 1: Latency âœ… EXCELLENT

```
Baseline: 173.8 Âµs/observation
Vajra:    12.5 Âµs/observation

Result: Vajra is 1289% faster (13x speedup)
```

**Verdict**: Performance advantage maintained after tuning.

#### Metric 2: Holographic Memory âœ… MAJOR FIX

```
Before Parameter Tuning: -93.7% (made recognition WORSE)
After Parameter Tuning:  +16.8% (improves recognition)

Result: Memory now HELPS instead of hurting
```

**Verdict**: This is a huge improvement! Memory went from broken to functional.

**Root Cause**: The reduced sensor modification (from alpha tuning) means memory can now match patterns more accurately. Previously, the Sheaf was distorting sensors so much that memory couldn't recognize trained patterns.

#### Metric 3: Dharma Ethics âœ… WORKING

```
Result: âœ… Dharma modified action (diff=0.68 BPM)
```

**Verdict**: Phase-based ethical veto continues to work correctly.

#### Metric 4: Sheaf Consensus âš ï¸ SLIGHT DEGRADATION

```
Baseline: Entropy 1.492, Confidence 0.501
Vajra:    Entropy 1.508, Confidence 0.539

Result: âŒ Sheaf INCREASES ambiguity by 1.1%
```

**Verdict**: Minor degradation. The reduced alpha means less aggressive consensus, which can leave more ambiguity in contradictory scenarios.

**Trade-off**: We prioritized sensor preservation over consensus strength. This trade-off is acceptable because:
- 1.1% is a minimal increase
- Sensor accuracy is more important than forced consensus
- Downstream belief engine can handle slightly higher entropy

#### Metric 5: Convergence Speed âŒ SIGNIFICANTLY WORSE

```
Baseline: Converged at 2 observations
Vajra:    Converged at 999 observations

Result: Vajra takes 500x longer to converge
```

**Verdict**: This is concerning. The reduced alpha makes the diffusion process more conservative, which slows convergence dramatically.

**Analysis**:
- Lower alpha â†’ smaller diffusion steps â†’ more iterations needed
- This test uses 1000 observations, so convergence at 999 means it barely converged
- May need to adjust convergence criteria or adaptive step sizing

---

## BEFORE vs AFTER COMPARISON

### Sensor Modification

| Metric | Before Fix | After Fix | After Tuning | Status |
|--------|-----------|-----------|--------------|--------|
| HR Change | +52% (wrong) | +36% | +23% | âœ… Improving |
| HRV Change | +42% (wrong) | +23% | +14% | âœ… Improving |
| RR Change | +200% (wrong) | +16% | +9% | âœ… Improving |
| Sheaf Energy | 5435.931 | 0.257 | 0.257 | âœ… Stable |

### Overall System Metrics

| Metric | Before Fix | After Fix | After Tuning |
|--------|-----------|-----------|--------------|
| Mode Accuracy | âŒ Always Energize | âœ… Correct mode | âœ… Correct mode |
| Memory | âŒ -93.7% | âŒ -93.7% | âœ… +16.8% |
| Latency | âœ… 50x faster | âœ… 50x faster | âœ… 13x faster |
| Convergence | Unknown | Unknown | âŒ 500x slower |

---

## ARCHITECTURAL LESSONS LEARNED

### 1. **Parameter Tuning is a Multi-Objective Optimization**

We improved sensor preservation (+44%) and memory recognition (+110 percentage points!), but degraded convergence speed (500x slower). This is a classic trade-off scenario.

### 2. **Memory Performance is Sensitive to Sensor Accuracy**

The memory went from -93.7% to +16.8% simply by reducing sensor modification. This shows that holographic recall requires precise pattern matching - even moderate sensor distortion (Â±20-36%) breaks the associative memory.

### 3. **Alpha Parameter Has Non-Linear Effects**

Reducing alpha from 0.05 to 0.02 (60% reduction):
- Reduced sensor modification by ~40%
- Reduced convergence speed by ~50000% (!)

This suggests alpha is in a sensitive range. Further tuning may benefit from logarithmic search (0.01, 0.015, 0.02, 0.025, etc.)

### 4. **Fair Testing Reveals Hidden Problems**

The convergence speed issue only appeared in the fair benchmark (1000 observations). Short tests (10-50 observations) would have missed this critical problem.

---

## RECOMMENDATIONS

### Immediate (Next Session) ğŸ”´ CRITICAL

1. **Fix Convergence Speed**:
   - Investigate adaptive step sizing in `diffuse_adaptive()` (sheaf.rs:159-172)
   - Consider increasing max_steps from 50 to reduce early stopping
   - May need to use different alpha values for different energy ranges

2. **Validate Sensor Modification Trade-off**:
   - Test with real physiological data to determine acceptable modification range
   - Current Â±9-23% may still be too high for some sensors
   - Establish ground truth with calibrated sensor data

### Short-Term (Next 2 Weeks)

3. **Recalibrate BeliefEngine**:
   - Adjust FEP priors for Sheaf-filtered distributions
   - The baseline expects raw sensors, but Vajra provides filtered sensors
   - May need separate agent thresholds (Gemini, MinhGioi, PhaQuan) for Vajra path

4. **Further Alpha Tuning**:
   - Test alpha values: [0.01, 0.015, 0.02, 0.025, 0.03]
   - Measure 4 metrics: sensor modification, memory accuracy, convergence speed, latency
   - Find Pareto optimal point

5. **Adaptive Alpha**:
   - Consider energy-dependent alpha: high energy â†’ larger alpha (faster consensus)
   - Low energy â†’ smaller alpha (preserve accurate sensors)
   - Implementation: `alpha = base_alpha * (1.0 + energy.sqrt())`

### Long-Term (1 Month)

6. **Full Skandha Integration**:
   - Current state: Only Rupa (Sheaf) is tuned
   - Need to integrate and tune: VedanÄ, SaÃ±Ã±Ä, Saá¹ƒskÄra, ViÃ±Ã±Äá¹‡a
   - Unified belief update path

7. **Production Validation**:
   - A/B test Vajra vs Baseline with real sensor data
   - Measure: Accuracy, latency, user satisfaction, battery usage
   - Gradual rollout if metrics improve

---

## NEXT STEPS

### Option A: Fix Convergence (Recommended)

**Goal**: Restore reasonable convergence speed without sacrificing sensor preservation

**Approach**:
1. Implement adaptive alpha based on energy
2. Increase max diffusion steps from 50 to 100
3. Adjust convergence detection threshold

**Timeline**: 1-2 days

### Option B: Accept Trade-off and Tune BeliefEngine

**Goal**: Work around slow convergence by improving belief update logic

**Approach**:
1. Accept that Vajra converges slower (more careful)
2. Optimize BeliefEngine to work with Sheaf-filtered distributions
3. Reduce required confidence threshold for action

**Timeline**: 3-5 days

### Option C: Hybrid Approach (Safest)

**Goal**: Use baseline for fast convergence, Vajra for precision

**Approach**:
1. First 10 observations: use baseline (fast bootstrap)
2. After 10 observations: switch to Vajra (precise steady-state)
3. Fallback to baseline if Vajra takes >100 observations

**Timeline**: 2-3 days

**Recommendation**: Start with Option A. If convergence can't be fixed without sacrificing sensor preservation, move to Option C.

---

## FILES MODIFIED

1. âœ… `crates/zenb-core/src/perception/sheaf.rs:115-116` - Tuned alpha and threshold
2. âœ… `crates/zenb-core/src/tests_proptest.rs:10` - Fixed Variable import
3. âœ… `SKANDHA_PARAMETER_TUNING_REPORT.md` - This report (new file)

---

## CONCLUSION

### What We Achieved

âœ… **Sensor Modification**: Reduced from Â±20-36% to Â±9-23% (up to 44% improvement)
âœ… **Memory Performance**: Fixed from -93.7% to +16.8% (110 point swing!)
âœ… **Latency**: Maintained 13x speedup
âœ… **Mode Accuracy**: No regressions, still correct

### Current Status

| Component | Status | Notes |
|-----------|--------|-------|
| **Sheaf Consensus** | âš ï¸ **Working** | 1.1% worse ambiguity (acceptable) |
| **Sensor Preservation** | âœ… **Improved** | Â±9-23% modification (down from Â±20-36%) |
| **Holographic Memory** | âœ… **Fixed** | Now helps +16.8% (was -93.7%) |
| **Convergence** | âŒ **Degraded** | 500x slower (needs fixing) |
| **Performance** | âœ… **Excellent** | Still 13x faster than baseline |

### Is Skandha Pipeline Worth It?

**Current Answer**: **YES, but needs convergence fix**

**Pros**:
- âœ… Memory now works (huge win!)
- âœ… Sensor modification acceptable (Â±9-23%)
- âœ… Still 13x faster than baseline
- âœ… Sheaf math is correct and tunable

**Cons**:
- âŒ Convergence speed unacceptable (500x slower)
- âš ï¸ May need BeliefEngine recalibration
- âš ï¸ Requires further tuning and validation

**Recommendation**:
1. **Fix convergence speed** (Option A: adaptive alpha)
2. Re-run benchmarks to verify no regressions
3. If convergence fixed â†’ proceed to BeliefEngine tuning
4. If convergence unfixable â†’ use hybrid approach (Option C)

---

**Next Git Commit**: Push parameter tuning changes and report

ğŸœ‚ **Eidolon Architect Prime**
*"Continuous improvement through empirical validation."*
