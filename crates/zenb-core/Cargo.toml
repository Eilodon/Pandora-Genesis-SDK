[package]
name = "zenb-core"
version = "0.1.0"
edition = "2021"

[dependencies]
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
thiserror = "1.0"
blake3 = "1.3"
uuid = { version = "1", features = ["v4"] }
chrono = { version = "0.4", features = ["serde"] }
config = "0.13"
toml = "0.8"
lru = "0.12"
nalgebra = "0.32"  # Linear algebra for UKF
log = "0.4"        # Logging
parking_lot = "0.12"  # High-performance mutex for circuit breaker

# Vajra-001: Holographic Memory dependencies
rustfft = "6.2"       # FFT processing for holographic memory
num-complex = { version = "0.4", features = ["serde"] }   # Complex number arithmetic + serialization
rand = "0.8"          # Random vector generation

# Vajra-Void: Signal processing integration
zenb-signals = { path = "../zenb-signals", optional = true }
ndarray = { version = "0.15", optional = true }  # For array processing in vajra_void

# SDK PORT: Monitoring dependencies
lazy_static = "1.4"  # Thread-safe metric singletons

[dependencies.prometheus]
version = "0.13"
optional = true

# LLM Integration: HTTP client for Ollama REST API
[dependencies.ureq]
version = "2.9"
optional = true
features = ["json"]

# GPU acceleration (cubecl) - WebGPU backend (most portable)
[dependencies.cubecl]
version = "0.8"
optional = true

[dependencies.cubecl-wgpu]
version = "0.8"
optional = true

[dev-dependencies]
tempfile = "3.10"
criterion = "0.5"     # Benchmarking for performance tests
proptest = "1.4"      # Property-based testing for invariant validation

# EIDOLON FIX 3.1, 3.3: Optional features for experimental/debug subsystems
[features]
default = []
# Skandha pipeline: Buddhist-inspired cognitive architecture (debug/visualization mode)
skandha_pipeline = []
# Thermodynamic logic: GENERIC framework for belief evolution (experimental)
thermodynamic_logic = []
# Allow dharma key mutation (test/research only - unsafe for production)
allow_dharma_mutation = []
# Vajra-Void: Enable advanced signal processing with EnsembleProcessor
vajra_void = ["zenb-signals", "ndarray"]
# SDK PORT: Prometheus metrics for production observability
prometheus = ["dep:prometheus"]
# VAJRA-VOID: Debug-only safety checks (Krylov energy monitoring, etc.)
vajra_debug = []
# Serde serialization support for breath patterns and other types
serde = []
# GPU FFT: WebGPU backend (portable: Windows/Linux/macOS/Web)
gpu_wgpu = ["cubecl", "cubecl-wgpu"]
# GPU FFT: CUDA backend for NVIDIA GPUs - uses cubecl-cuda
gpu_cuda = ["cubecl"]
# GPU FFT: Metal backend for Apple GPUs - uses cubecl-wgpu on macOS
gpu_metal = ["gpu_wgpu"]
# LLM Integration: Ollama local provider (DeepSeek R1, Llama, etc.)
llm-ollama = ["dep:ureq"]
# LLM Integration: OpenAI-compatible API provider (supports local servers too)
llm-openai = ["dep:ureq"]
# LLM Integration: llama.cpp server provider
llm-llama = ["dep:ureq"]
# LLM Integration: All providers
llm-all = ["llm-ollama", "llm-openai", "llm-llama"]

[[bench]]
name = "forensic_bench"
harness = false

[[example]]
name = "b1_v3_demonstration"
path = "examples/b1_v3_demonstration.rs"
