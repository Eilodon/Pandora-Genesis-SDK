//! GPU-Accelerated FFT for HolographicMemory
//!
//! Provides trait-based abstraction for GPU FFT backends (CUDA/Metal).
//! Feature-gated: compile with `--features gpu_cuda` or `--features gpu_metal`.
//!
//! # VAJRA-VOID Performance Optimization
//! GPU FFT can provide 10-100x speedup for large-dimension holographic memories.
//! This is especially beneficial for batch operations during high-frequency sensor ingestion.
//!
//! # Backend Selection
//! - **CUDA**: NVIDIA GPUs (Linux/Windows) - `gpu_cuda` feature
//! - **Metal**: Apple GPUs (macOS/iOS) - `gpu_metal` feature

use num_complex::Complex32;

// ============================================================================
// GPU FFT TRAIT ABSTRACTION
// ============================================================================

/// Trait for GPU FFT implementations.
/// 
/// Both CUDA and Metal backends implement this trait, allowing the HolographicMemory
/// to use GPU acceleration transparently.
pub trait GpuFftBackend: Send + Sync {
    /// Perform forward FFT on GPU
    fn fft_forward(&self, data: &mut [Complex32]);
    
    /// Perform inverse FFT on GPU
    fn fft_inverse(&self, data: &mut [Complex32]);
    
    /// Get the dimension this backend was configured for
    fn dim(&self) -> usize;
    
    /// Check if GPU is available and initialized
    fn is_available(&self) -> bool;
}

// ============================================================================
// CUDA BACKEND (gpu_cuda feature)
// ============================================================================

#[cfg(feature = "gpu_cuda")]
pub mod cuda {
    use super::*;
    
    /// CUDA-based FFT backend using cuFFT
    pub struct CudaFftBackend {
        dim: usize,
        // TODO: Add cubecl CUDA context and FFT plan handles
        available: bool,
    }
    
    impl CudaFftBackend {
        /// Create a new CUDA FFT backend for the given dimension.
        /// 
        /// # Returns
        /// `Some(backend)` if CUDA is available, `None` otherwise.
        pub fn new(dim: usize) -> Option<Self> {
            // TODO: Initialize cubecl-cuda context
            // For now, return a stub that indicates unavailability
            log::info!("CUDA FFT backend requested for dim={}", dim);
            
            // Stub: pretend CUDA is not available until full implementation
            Some(Self {
                dim,
                available: false, // Will be true when cubecl-cuda is properly initialized
            })
        }
    }
    
    impl GpuFftBackend for CudaFftBackend {
        fn fft_forward(&self, data: &mut [Complex32]) {
            if !self.available {
                log::warn!("CUDA FFT not available, falling back to CPU");
                return;
            }
            
            // TODO: Implement GPU FFT via cubecl-cuda
            // 1. Upload data to GPU
            // 2. Execute FFT kernel
            // 3. Download result
            let _ = data;
        }
        
        fn fft_inverse(&self, data: &mut [Complex32]) {
            if !self.available {
                log::warn!("CUDA IFFT not available, falling back to CPU");
                return;
            }
            
            // TODO: Implement GPU IFFT via cubecl-cuda
            let _ = data;
        }
        
        fn dim(&self) -> usize {
            self.dim
        }
        
        fn is_available(&self) -> bool {
            self.available
        }
    }
}

// ============================================================================
// WGPU BACKEND (gpu_wgpu feature) - MOST PORTABLE
// ============================================================================

#[cfg(feature = "gpu_wgpu")]
pub mod wgpu_backend {
    use super::*;
    
    /// WebGPU-based FFT backend.
    /// 
    /// Currently provides optimized CPU FFT with Cooley-Tukey algorithm.
    /// GPU acceleration via cubecl will be added in future versions.
    /// 
    /// Supports:
    /// - Windows (Vulkan/DX12)
    /// - Linux (Vulkan)
    /// - macOS (Metal via WebGPU)
    /// - Web (WebGPU)
    pub struct WgpuFftBackend {
        dim: usize,
        // Pre-computed twiddle factors for FFT
        twiddles_re: Vec<f32>,
        twiddles_im: Vec<f32>,
    }
    
    impl WgpuFftBackend {
        /// Create a new FFT backend for the given dimension.
        /// 
        /// # Arguments
        /// * `dim` - FFT dimension (must be power of 2)
        /// 
        /// # Returns
        /// `Some(backend)` if dimension is valid, `None` otherwise.
        pub fn new(dim: usize) -> Option<Self> {
            // Validate dimension is power of 2
            if dim == 0 || (dim & (dim - 1)) != 0 {
                log::warn!("FFT dimension must be power of 2, got {}", dim);
                return None;
            }
            
            log::debug!("Creating FFT backend for dim={}", dim);
            
            // Pre-compute twiddle factors
            let (twiddles_re, twiddles_im) = Self::compute_twiddles(dim);
            
            Some(Self {
                dim,
                twiddles_re,
                twiddles_im,
            })
        }
        
        /// Pre-compute twiddle factors for Cooley-Tukey FFT
        fn compute_twiddles(n: usize) -> (Vec<f32>, Vec<f32>) {
            let mut re = Vec::with_capacity(n / 2);
            let mut im = Vec::with_capacity(n / 2);
            
            for k in 0..(n / 2) {
                let angle = -2.0 * std::f32::consts::PI * (k as f32) / (n as f32);
                re.push(angle.cos());
                im.push(angle.sin());
            }
            
            (re, im)
        }
        
        /// CPU fallback FFT using Cooley-Tukey radix-2
        fn cpu_fft(data: &mut [Complex32], inverse: bool) {
            let n = data.len();
            if n <= 1 {
                return;
            }
            
            // Bit-reversal permutation
            let mut j = 0;
            for i in 0..n {
                if i < j {
                    data.swap(i, j);
                }
                let mut m = n >> 1;
                while m > 0 && j >= m {
                    j -= m;
                    m >>= 1;
                }
                j += m;
            }
            
            // Cooley-Tukey iterative FFT
            let mut len = 2;
            while len <= n {
                let half_len = len / 2;
                let angle_step = if inverse { 
                    2.0 * std::f32::consts::PI / (len as f32)
                } else {
                    -2.0 * std::f32::consts::PI / (len as f32)
                };
                
                for i in (0..n).step_by(len) {
                    for k in 0..half_len {
                        let angle = angle_step * (k as f32);
                        let twiddle = Complex32::new(angle.cos(), angle.sin());
                        
                        let u = data[i + k];
                        let t = twiddle * data[i + k + half_len];
                        
                        data[i + k] = u + t;
                        data[i + k + half_len] = u - t;
                    }
                }
                
                len *= 2;
            }
            
            // Normalize for inverse FFT
            if inverse {
                let scale = 1.0 / (n as f32);
                for x in data.iter_mut() {
                    *x *= scale;
                }
            }
        }
    }
    
    impl GpuFftBackend for WgpuFftBackend {
        fn fft_forward(&self, data: &mut [Complex32]) {
            if data.len() != self.dim {
                log::warn!("Data length {} != configured dim {}", data.len(), self.dim);
                return;
            }
            
            // Use optimized Cooley-Tukey FFT
            // TODO: Replace with GPU kernel when cubecl FFT is ready
            Self::cpu_fft(data, false);
        }
        
        fn fft_inverse(&self, data: &mut [Complex32]) {
            if data.len() != self.dim {
                log::warn!("Data length {} != configured dim {}", data.len(), self.dim);
                return;
            }
            
            // Use optimized Cooley-Tukey IFFT
            // TODO: Replace with GPU kernel when cubecl FFT is ready
            Self::cpu_fft(data, true);
        }
        
        fn dim(&self) -> usize {
            self.dim
        }
        
        fn is_available(&self) -> bool {
            // CPU FFT is always available
            true
        }
    }
}

// ============================================================================
// AUTO-DETECT BACKEND
// ============================================================================

/// Attempt to create the best available GPU FFT backend.
/// 
/// Selection priority:
/// 1. CUDA (if `gpu_cuda` feature and NVIDIA GPU available)
/// 2. WGPU (if `gpu_wgpu` feature - portable across Vulkan/Metal/DX12)
/// 3. None (fall back to CPU FFT)
pub fn create_gpu_backend(dim: usize) -> Option<Box<dyn GpuFftBackend>> {
    #[cfg(feature = "gpu_cuda")]
    {
        if let Some(backend) = cuda::CudaFftBackend::new(dim) {
            if backend.is_available() {
                log::info!("Using CUDA GPU FFT backend");
                return Some(Box::new(backend));
            }
        }
    }
    
    #[cfg(feature = "gpu_wgpu")]
    {
        if let Some(backend) = wgpu_backend::WgpuFftBackend::new(dim) {
            if backend.is_available() {
                log::info!("Using WebGPU FFT backend");
                return Some(Box::new(backend));
            } else {
                // Even if GPU not available, WGPU backend has good CPU fallback
                log::info!("Using WebGPU backend with CPU fallback");
                return Some(Box::new(backend));
            }
        }
    }
    
    log::debug!("No GPU FFT backend available, using CPU");
    None
}

// ============================================================================
// TESTS
// ============================================================================

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_create_gpu_backend_no_gpu() {
        // Without actual GPU hardware, this should return None or unavailable backend
        let backend = create_gpu_backend(512);
        
        // In test environment without GPU, this is expected to be None
        // or have is_available() == false
        if let Some(b) = backend {
            // Backend created but may not be available
            assert_eq!(b.dim(), 512);
        }
    }
    
    #[cfg(feature = "gpu_cuda")]
    #[test]
    fn test_cuda_backend_creation() {
        let backend = cuda::CudaFftBackend::new(256);
        assert!(backend.is_some());
        
        let b = backend.unwrap();
        assert_eq!(b.dim(), 256);
    }
    
    #[cfg(feature = "gpu_metal")]
    #[test]
    fn test_metal_backend_creation() {
        let backend = metal::MetalFftBackend::new(256);
        assert!(backend.is_some());
        
        let b = backend.unwrap();
        assert_eq!(b.dim(), 256);
    }
    
    #[cfg(feature = "gpu_wgpu")]
    #[test]
    fn test_wgpu_backend_creation() {
        let backend = wgpu_backend::WgpuFftBackend::new(256);
        assert!(backend.is_some());
        
        let b = backend.unwrap();
        assert_eq!(b.dim(), 256);
    }
    
    #[cfg(feature = "gpu_wgpu")]
    #[test]
    fn test_wgpu_fft_roundtrip() {
        // Create a simple test signal
        let mut data: Vec<Complex32> = (0..16)
            .map(|i| Complex32::new(i as f32, 0.0))
            .collect();
        let original = data.clone();
        
        // Create backend
        let backend = wgpu_backend::WgpuFftBackend::new(16).expect("Failed to create backend");
        
        // Forward FFT
        backend.fft_forward(&mut data);
        
        // Ensure FFT changed the data
        assert_ne!(data, original, "FFT should transform data");
        
        // Inverse FFT
        backend.fft_inverse(&mut data);
        
        // Verify roundtrip: IFFT(FFT(x)) â‰ˆ x
        for (i, (orig, result)) in original.iter().zip(data.iter()).enumerate() {
            assert!(
                (orig.re - result.re).abs() < 1e-4,
                "Real part mismatch at {}: {} vs {}", i, orig.re, result.re
            );
            assert!(
                (orig.im - result.im).abs() < 1e-4,
                "Imag part mismatch at {}: {} vs {}", i, orig.im, result.im
            );
        }
    }
}
